{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "!<img src=\"transformer_architecture.png\" alt=\"Transformer Architecture\" width=\"500\"/>",
   "id": "2206b3636e339241"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A classic RNN $$ h_t = W_{xh} \\odot x + W_{hh} \\odot h_{t-1) $$ $$ y_t = W_{hy} \\odot h_t $$\n",
    "\n",
    "This has infinite short-term memory. Attempts have appeared with LSTM and GRU to fix it, but it seems like today the best way is self-attention.\n",
    "\n",
    "**Self-attention** has, by definition, bounded memory to the size of the sequence length   \n",
    "and now the inputs are not passed through hidden layers sequentially,  \n",
    "instead are passed through a self-attention layer as a sequence with positional embeddings.\n",
    "\n",
    "**Attention Mechanism** can be though of as a memory with keys and values and a layer  \n",
    " which, when someone queries it, generates an output from value whose keys map the input.\n",
    " \n",
    "The formulas are simple:\n",
    "- In Bahdanau Seq2Seq there is no $\\mathbf{v}$ vector for values, the $\\mathbf{v}$ is a linear projection matrix (energies_layer) - \n",
    "$$ output = \\text{softmax}(\\textbf{a}(\\mathbf{q}, \\mathbf{k})) \\cdot \\mathbf{v} $$ \n",
    "\n",
    "where $ \\textbf{a} $ can be as simple as $ \\alpha(\\mathbf{q}, \\mathbf{k}) = \\mathbf{q} \\cdot \\mathbf{k} $ \n",
    "or a projection to a hidden dimension using $ \\left(\\mathbf{W_k}, \\mathbf{W_q} \\right) $ so now we end up with \n",
    "$$ \\alpha(\\mathbf{q}, \\mathbf{k}) = \\mathbf{v}^\\mathsf{T} \\text{tanh}(\\mathbf{W_k}\\mathbf{k} + \\mathbf{W_q}\\mathbf{q}) $$\n",
    "\n",
    "To maintain positional information we use positional encodings $$ P_{i, 2j} = \\text{sin}(i/1000^{2j/d})   P_{i, 2j+1} = \\text{cos}(i/1000^{2j/d}) $$\n",
    "\n",
    "!<img src=\"positional_encoding_visualization.png\" alt=\"Positional Encoding for Different Depths\" width=\"500\"/>\n",
    "\n",
    "And so we end up with a new input representation where we combine the token embeddings $ W $ and the positional ones: $$ \\mathbf{X} = \\mathbf{W} + \\mathbf{P} $$\n",
    " \n"
   ],
   "id": "e9e56149ac34b9e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The Transformer body\n",
    "\n",
    "**self-attention**:\n",
    "$ \\text{attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^\\mathsf{T}} {\\sqrt{d_k}}\\right) \\mathbf{V} $\n",
    "\n",
    "\n",
    "And when we use multiple self-attention heads to capture different meanings we have multi-head attention, where: \n",
    "$$ head_i = attention(\\mathbf{W_q^i}\\mathbf{Q}, \\mathbf{W_k^i}\\mathbf{K}, \\mathbf{W_v^i}\\mathbf{V}) $$\n",
    "$$ multihead(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\mathbf{W_0}\\text{concat}(head_1,..., head_h)$$\n",
    "\n",
    "\n",
    "**Masked Attention:** For the decoder to learn we hide future inputs by adding M where M is -inf for future values.\n",
    "$$ \\text{maskedAttention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^\\mathsf{T} + \\mathbf{M}} {\\sqrt{d_k}}\\right) \\mathbf{V} $$\n",
    "\n",
    "\n",
    "**AddAndNorm and ResidualConnections**\n",
    "As we can see in the transformer image, we use a skip-connection to bypass the inputs $X$ to the output $Z$ -like in ResNets- where they are added and normalized to mean 0 and std 1 $h_i = \\frac{g_{ain}}{\\sigma}(h_i - \\mu)$.  \n",
    "\n",
    "**Positionwise FFNs**: They are kind of unique... (Similar linear transformations with ReLU activation in between is performed)\n",
    "$$ FFN(\\mathbf{x}) = \\text{ReLU}(\\mathbf{xW_1} + b_1)\\mathbf{W_2} + b_2 $$ \n",
    "\n",
    "**Encoder-Decoder** As in image\n"
   ],
   "id": "58692f26fe84ea31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Machine Translation\n",
    "\n",
    "## First, we create a Bahdanau Seq2Seq Bidirectional GRU with attention\n",
    "Update Gate:\n",
    "$ z_t^{(f)} = \\sigma(W_z^{(f)} \\cdot [h_{t-1}^{(f)}, x_t]) $\n",
    "$ z_t^{(b)} = \\sigma(W_z^{(b)} \\cdot [h_{t+1}^{(b)}, x_t]) $  \n",
    "Reset Gate:\n",
    "$ r_t^{(f)} = \\sigma(W_r^{(f)} \\cdot [h_{t-1}^{(f)}, x_t]) $\n",
    "$ r_t^{(b)} = \\sigma(W_r^{(b)} \\cdot [h_{t+1}^{(b)}, x_t]) $  \n",
    "Hidden State:\n",
    "$ \\tilde{h}_t^{(f)} = \\tanh(W_h^{(f)} \\cdot [r_t^{(f)} \\odot h_{t-1}^{(f)}, x_t]) $\n",
    "$ h_t^{(f)} = (1 - z_t^{(f)}) \\odot h_{t-1}^{(f)} + z_t^{(f)} \\odot \\tilde{h}_t^{(f)} $    \n",
    "$ \\tilde{h}_t^{(b)} = \\tanh(W_h^{(b)} \\cdot [r_t^{(b)} \\odot h_{t+1}^{(b)}, x_t]) $\n",
    "$ h_t^{(b)} = (1 - z_t^{(b)}) \\odot h_{t+1}^{(b)} + z_t^{(b)} \\odot \\tilde{h}_t^{(b)} $\n",
    "\n",
    "**Combined Hidden State (concat)**\n",
    "$ h_t = [h_t^{(f)}, h_t^{(b)}] $"
   ],
   "id": "160b2c6662aa46c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T22:25:32.365213Z",
     "start_time": "2024-06-27T22:25:32.148619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ],
   "id": "3482b9227ecd3f5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T20:21:08.449780Z",
     "start_time": "2024-06-27T20:21:06.173624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Splitting data into train, validation, and test sets\n",
    "SPECIALS = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "tokenized_data_path = 'tokenized_data.pt'\n",
    "en_vocab_path = 'en_vocab.pkl'\n",
    "fr_vocab_path = 'fr_vocab.pkl'\n",
    "\n",
    "with open(en_vocab_path, 'rb') as f:\n",
    "    en_vocab = pickle.load(f)\n",
    "\n",
    "with open(fr_vocab_path, 'rb') as f:\n",
    "    fr_vocab = pickle.load(f)\n",
    "\n",
    "en_data, fr_data = torch.load(tokenized_data_path)\n",
    "\n",
    "VALID_PCT = 0.1\n",
    "TEST_PCT = 0.1\n",
    "\n",
    "train_data = []\n",
    "valid_data = []\n",
    "test_data = []\n",
    "\n",
    "random.seed(6547)\n",
    "for en_tensor_, fr_tensor_ in zip(en_data, fr_data):\n",
    "    en_tensor_ = torch.tensor(en_tensor_)\n",
    "    fr_tensor_ = torch.tensor(fr_tensor_)\n",
    "    random_draw = random.random()\n",
    "    if random_draw <= VALID_PCT:\n",
    "        valid_data.append((en_tensor_, fr_tensor_))\n",
    "    elif random_draw <= VALID_PCT + TEST_PCT:\n",
    "        test_data.append((en_tensor_, fr_tensor_))\n",
    "    else:\n",
    "        train_data.append((en_tensor_, fr_tensor_))\n",
    "\n",
    "print(f\"\"\"\n",
    "  Training pairs: {len(train_data):,}\n",
    "Validation pairs: {len(valid_data):,}\n",
    "      Test pairs: {len(test_data):,}\"\"\")\n",
    "\n",
    "# Define special tokens indices\n",
    "PAD_IDX = en_vocab['<pad>']\n",
    "BOS_IDX = en_vocab['<bos>']\n",
    "EOS_IDX = en_vocab['<eos>']\n",
    "\n",
    "# Ensure that special tokens are the same in both vocabularies\n",
    "for en_id, fr_id in zip([en_vocab[token] for token in SPECIALS], [fr_vocab[token] for token in SPECIALS]):\n",
    "    assert en_id == fr_id\n",
    "\n",
    "\n",
    "# Function to generate a batch of data\n",
    "def generate_batch(data_batch):\n",
    "    en_batch, fr_batch = [], []\n",
    "    for (en_item, fr_item) in data_batch:\n",
    "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "        fr_batch.append(torch.cat([torch.tensor([BOS_IDX]), fr_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    fr_batch = pad_sequence(fr_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "\n",
    "    return en_batch, fr_batch\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "valid_iter = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch)\n",
    "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch)\n",
    "\n",
    "# Verify the DataLoader output\n",
    "for i, (en_id, fr_id) in enumerate(train_iter):\n",
    "    print('\\nEnglish:', ' '.join([list(en_vocab.keys())[list(en_vocab.values()).index(idx)] for idx in en_id[:, 0]]))\n",
    "    print('French:', ' '.join([list(fr_vocab.keys())[list(fr_vocab.values()).index(idx)] for idx in fr_id[:, 0]]))\n",
    "    if i == 4: break\n"
   ],
   "id": "32558a786df1fc3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Training pairs: 108,111\n",
      "Validation pairs: 13,648\n",
      "      Test pairs: 13,525\n",
      "\n",
      "English: <bos> this time you ve gone too far . <eos> <pad> <pad>\n",
      "French: <bos> cette fois tu es alle trop loin . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "English: <bos> open up . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "French: <bos> ouvre moi ! <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "English: <bos> do you think that eating breakfast every day is important ? <eos> <pad>\n",
      "French: <bos> penses tu que prendre un petit dejeuner chaque jour soit important ? <eos> <pad> <pad> <pad> <pad>\n",
      "\n",
      "English: <bos> how tall you are ! <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "French: <bos> comme tu es grand ! <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "English: <bos> it keeps you on your toes . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "French: <bos> ca t oblige a rester vigilante . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T22:17:16.370761Z",
     "start_time": "2024-06-27T22:17:16.362365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_SENTENCE_LENGTH = 20\n",
    "FILTER_TO_BASIC_PREFIXES = False\n",
    "SAVE_DIR = os.path.join(\".\", \"simple_models\")\n",
    "\n",
    "ENCODER_EMBEDDING_DIM = 256\n",
    "ENCODER_HIDDEN_SIZE = 256\n",
    "DECODER_EMBEDDING_DIM = 256\n",
    "DECODER_HIDDEN_SIZE = 256"
   ],
   "id": "a816c11493fee83f",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T21:02:44.551935Z",
     "start_time": "2024-06-27T21:02:44.536058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BahdanauEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, encoder_hidden_dim, decoder_hidden_dim, dropout_p):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim  # ~= Vocabulary size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.encoder_hidden_dim = encoder_hidden_dim\n",
    "        self.decoder_hidden_dim = decoder_hidden_dim\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # (B,Voc) -> (B, Voc, Emb) The weights the multiply (1,Voc) vectors follow U(-embedding_dim^-1/2, embedding_dim^-1/2)  \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, encoder_hidden_dim, bidirectional=True)\n",
    "        self.linear = nn.Linear(encoder_hidden_dim * 2,\n",
    "                                decoder_hidden_dim)  # Maps concatenated hidden states (from both directions) to the decoder's hidden.\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        # The embedded sequence is processed by the bidirectional GRU. \n",
    "        # The outputs contain the hidden states for all time steps, while hidden contains the final hidden states from both directions.\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "\n",
    "        \"\"\"Syntax Breakdown:\n",
    "        torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1):\n",
    "\n",
    "        hidden[-2, :, :] refers to the last hidden state from the backward GRU.\n",
    "        hidden[-1, :, :] refers to the last hidden state from the forward GRU.\n",
    "        torch.hstack(...) concatenates these two hidden states along the feature dimension, resulting in a tensor of size [batch_size, encoder_hidden_dim * 2].\n",
    "        self.linear(...):\n",
    "\n",
    "        The concatenated hidden states are passed through the linear layer to map them to the decoder_hidden_dim.\"\"\"\n",
    "        hidden = torch.tanh(self.linear(torch.hstack((hidden[-2, :, :], hidden[-1, :, :]))))\n",
    "        return outputs, hidden\n"
   ],
   "id": "1c402baf23e692db",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "!<img src=\"seq2seq.png\">>\n",
    "1. The output of the last encoder states are used as keys $\\mathbf{k}$ and values $\\mathbf{v}$\n",
    "2. The output of the last decoder state, at time $t-1$ is used as query $\\mathbf{q}$\n",
    "3. The output from the attention layer $\\mathbf{o}$, the context variable, is used for the next decoder state $t$"
   ],
   "id": "c8aa565731f92c9b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-27T21:02:45.002680Z",
     "start_time": "2024-06-27T21:02:44.986694Z"
    }
   },
   "source": [
    "class BahdanauAttentionQKV(nn.Module):\n",
    "    def __init__(self, hidden_size, query_size=None, key_size=None, dropout_p=0.15):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.query_size = hidden_size if query_size is None else query_size\n",
    "\n",
    "        # assume bidirectional encoder, but can specify otherwise\n",
    "        self.key_size = 2 * hidden_size if key_size is None else key_size\n",
    "        self.query_layer = nn.Linear(self.query_size, self.hidden_size)\n",
    "        self.key_layer = nn.Linear(self.key_size, self.hidden_size)\n",
    "        self.energy_layer = nn.Linear(self.hidden_size, 1)  # score?\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, decoder_last_hidden, encoder_outputs, src_mask=None):\n",
    "        # (B, H)\n",
    "        query_out = self.query_layer(decoder_last_hidden)\n",
    "        # (Src, B, 2*H) --> (Src, B, H)\n",
    "        key_out = self.key_layer(encoder_outputs)\n",
    "        # (B, H) + (Src, B, H) = (Src, B ,H)\n",
    "        energy_input = torch.tanh(query_out + key_out)\n",
    "        # (Src, B, H) --> (Src, B, 1) --> (Src, B)\n",
    "        energies = self.energy_layer(energy_input).squeeze(2)  # todo could this be just squeeze()\n",
    "        # if a mask is provided, remove masked tokens from softmax calc\n",
    "        if src_mask is not None:\n",
    "            energies.data.masked_fill_(src_mask == 0, float(\"-inf\"))\n",
    "        # softmax over the length dimension\n",
    "        weights = F.softmax(energies,\n",
    "                            dim=0)  # So we now have the attention probs. We do softmax in batch dim so for every element.\n",
    "        # return as (B, Src) as expected by later multiplication\n",
    "        return weights.transpose(0, 1)"
   ],
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T22:30:49.642852Z",
     "start_time": "2024-06-27T22:30:49.618212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BahdanauDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, encoder_hidden_dim,\n",
    "                 decoder_hidden_dim, attention, dropout_p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.encoder_hidden_dim = encoder_hidden_dim\n",
    "        self.decoder_hidden_dim = decoder_hidden_dim\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.attention = attention  # allowing for custom attention\n",
    "        self.gru = nn.GRU((encoder_hidden_dim * 2) + embedding_dim,\n",
    "                          decoder_hidden_dim)\n",
    "        self.out = nn.Linear((encoder_hidden_dim * 2) + embedding_dim + decoder_hidden_dim,\n",
    "                             output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs, src_mask=None):\n",
    "        '''\n",
    "        Decode an encoder's output. \n",
    "\n",
    "        B: batch size\n",
    "        S: source sentence length\n",
    "        T: target sentence length\n",
    "        O: output size (target vocab size)\n",
    "        Enc: encoder hidden dim\n",
    "        Dec: decoder hidden dim\n",
    "        Emb: embedding dim\n",
    "\n",
    "        Inputs:\n",
    "          - input: a vector of length B giving the most recent decoded token\n",
    "          - hidden: a (B, Dec) most recent RNN hidden state\n",
    "          - encoder_outputs: (S, B, 2*Enc) sequence of outputs from encoder RNN\n",
    "\n",
    "        Outputs:\n",
    "          - output: logits for next token in the sequence (B, O)\n",
    "          - hidden: a new (B, Dec) RNN hidden state\n",
    "          - attentions: (B, S) attention weights for the current token over the source sentence\n",
    "        '''\n",
    "\n",
    "        # (B) --> (1, B)\n",
    "        input = input.unsqueeze(0)\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        attentions = self.attention(hidden, encoder_outputs, src_mask)\n",
    "\n",
    "        # (B, S) --> (B, 1, S)\n",
    "        a = attentions.unsqueeze(1)\n",
    "\n",
    "        # (S, B, 2*Enc) --> (B, S, 2*Enc)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)\n",
    "\n",
    "        # weighted encoder representation\n",
    "        # (B, 1, S) @ (B, S, 2*Enc) = (B, 1, 2*Enc)\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "\n",
    "        # (B, 1, 2*Enc) --> (1, B, 2*Enc)\n",
    "        weighted = weighted.transpose(0, 1)\n",
    "\n",
    "        # concat (1, B, Emb) and (1, B, 2*Enc)\n",
    "        # results in (1, B, Emb + 2*Enc)\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(rnn_input, hidden.unsqueeze(0))\n",
    "\n",
    "        assert (output == hidden).all()\n",
    "\n",
    "        # get rid of empty leading dimensions\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "\n",
    "        # concatenate the pieces above\n",
    "        # (B, Dec), (B, 2*Enc), and (B, Emb)\n",
    "        # result is (B, Dec + 2*Enc + Emb)\n",
    "        linear_input = torch.cat((output, weighted, embedded), dim=1)\n",
    "\n",
    "        # (B, Dec + 2*Enc + Emb) --> (B, O)\n",
    "        output = self.out(linear_input)\n",
    "\n",
    "        return output, hidden.squeeze(0), attentions"
   ],
   "id": "c6bab118684b659e",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T22:30:50.341634Z",
     "start_time": "2024-06-27T22:30:50.325244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class BahdanauDecoder(nn.Module):\n",
    "#     def __init__(self, output_dim, embedding_dim, encoder_hidden_dim,\n",
    "#                  decoder_hidden_dim, attention, dropout_p=0.15):\n",
    "#         super().__init__()\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.output_dim = output_dim\n",
    "#         self.encoder_hidden_dim = encoder_hidden_dim\n",
    "#         self.decoder_hidden_dim = decoder_hidden_dim\n",
    "#         self.dropout_p = dropout_p\n",
    "#         \n",
    "#         self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "#         self.attention = attention  # allowing for custom attention\n",
    "#         self.gru = nn.GRU((encoder_hidden_dim * 2) + embedding_dim, decoder_hidden_dim)     # Now only forward direction\n",
    "#         self.out = nn.Linear((encoder_hidden_dim * 2) + embedding_dim + decoder_hidden_dim, output_dim)\n",
    "#         self.dropout = nn.Dropout(dropout_p)\n",
    "#         \n",
    "#     def forward(self, input, dec_prev_hidden, encoder_outputs, src_mask=None):\n",
    "#         # (B) --> (1, B)\n",
    "#         input = input.unsqueeze(0)\n",
    "#         embedded = self.dropout(self.embedding(input))\n",
    "#         attentions = self.attention(dec_prev_hidden, encoder_outputs, src_mask)\n",
    "#         # (B, S) --> (B, 1, S)\n",
    "#         a = attentions.unsqueeze(1)\n",
    "#         # (S, B, 2*Enc) --> (B, S, 2*Enc)\n",
    "#         encoder_outputs = encoder_outputs.transpose(0, 1)\n",
    "#         # weighted encoder representation\n",
    "#         # (B, 1, S) @ (B, S, 2*Enc) = (B, 1, 2*Enc)\n",
    "#         weighted = torch.bmm(a, encoder_outputs)    # enc_out = values here\n",
    "#         # (B, 1, 2*Enc) --> (1, B, 2*Enc)\n",
    "#         weighted = weighted.transpose(0, 1)\n",
    "#         # concat (1, B, Emb) and (1, B, 2*Enc)\n",
    "#         # results in (1, B, Emb + 2*Enc)\n",
    "#         rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "#         output, dec_current_hidden = self.gru(rnn_input, dec_prev_hidden.unsqueeze(0))\n",
    "#         \n",
    "#         assert (output == dec_prev_hidden)\n",
    "#         \n",
    "#         # get rid of empty leading dimensions\n",
    "#         embedded = embedded.squeeze(0)\n",
    "#         output = output.squeeze(0)\n",
    "#         weighted = weighted.squeeze(0)\n",
    "#         \n",
    "#         # concatenate the pieces above\n",
    "#         # (B, Dec), (B, 2*Enc), and (B, Emb)\n",
    "#         # result is (B, Dec + 2*Enc + Emb)\n",
    "#         # To take the final decision we add two skip connections so that\n",
    "#         #   we do so w.r.t. dec_cur_out, weighted_enc_cur_out, targ_emb  \n",
    "#         linear_input = torch.cat((output, weighted, embedded), dim=1)\n",
    "#         # (B, Dec + 2*Enc + Emb) --> (B, O)\n",
    "#         output = self.out(linear_input)\n",
    "#         return output, dec_current_hidden(0), attentions"
   ],
   "id": "f8c7da8404ed5fe0",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T22:30:50.899398Z",
     "start_time": "2024-06-27T22:30:50.875865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BahdanauSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder.to(device)\n",
    "        self.decoder = decoder.to(device)\n",
    "        self.device = device\n",
    "        self.tgt_vocab_size = decoder.output_dim\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, teacher_forcing_ratio=0.5, return_attentions=False):\n",
    "\n",
    "        tgt_length, batch_size = tgt.shape\n",
    "\n",
    "        # store decoder outputs\n",
    "        outputs = torch.zeros(tgt_length, batch_size, self.tgt_vocab_size).to(self.device)\n",
    "        # attentions = torch.zeros(tgt_length, batch_size, )\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        hidden = hidden.squeeze(1)  # B, 1, Enc --> B, Enc (if necessary)\n",
    "\n",
    "        # start with <bos> as the decoder input\n",
    "        decoder_input = tgt[0, :]\n",
    "        attentions = []\n",
    "\n",
    "        for t in range(1, tgt_length):\n",
    "            decoder_output, hidden, attention = self.decoder(decoder_input, hidden, encoder_outputs, src_mask)\n",
    "            outputs[t] = decoder_output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top_token = decoder_output.max(1)[1]\n",
    "            decoder_input = (tgt[t] if teacher_force else top_token)\n",
    "            attentions.append(attention.unsqueeze(-1))\n",
    "\n",
    "        if return_attentions:\n",
    "            return outputs, torch.cat(attentions, dim=-1)\n",
    "        else:\n",
    "            return outputs"
   ],
   "id": "b3a233002751ec4c",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T22:30:51.782299Z",
     "start_time": "2024-06-27T22:30:51.476636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "enc = BahdanauEncoder(input_dim=len(en_vocab),\n",
    "                      embedding_dim=ENCODER_EMBEDDING_DIM,\n",
    "                      encoder_hidden_dim=ENCODER_HIDDEN_SIZE,\n",
    "                      decoder_hidden_dim=DECODER_HIDDEN_SIZE,\n",
    "                      dropout_p=0.15)\n",
    "\n",
    "attn = BahdanauAttentionQKV(DECODER_HIDDEN_SIZE)\n",
    "\n",
    "dec = BahdanauDecoder(output_dim=len(fr_vocab),\n",
    "                      embedding_dim=DECODER_EMBEDDING_DIM,\n",
    "                      encoder_hidden_dim=ENCODER_HIDDEN_SIZE,\n",
    "                      decoder_hidden_dim=DECODER_HIDDEN_SIZE,\n",
    "                      attention=attn,\n",
    "                      dropout_p=0.15)\n",
    "\n",
    "seq2seq = BahdanauSeq2Seq(enc, dec, device)"
   ],
   "id": "e37149ce7115a723",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T22:30:51.797678Z",
     "start_time": "2024-06-27T22:30:51.782299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultipleOptimizer(object):\n",
    "    def __init__(self, *op):\n",
    "        self.optimizers = op\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for op in self.optimizers:\n",
    "            op.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        for op in self.optimizers:\n",
    "            op.step()"
   ],
   "id": "423f5858a40b8e3a",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T22:30:52.022350Z",
     "start_time": "2024-06-27T22:30:52.001924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, iterator, optimizer, loss_fn, device, clip=None):\n",
    "    model.train()\n",
    "    if model.device != device:\n",
    "        model = model.to(device)\n",
    "\n",
    "    epoch_loss = 0\n",
    "    with tqdm(total=len(iterator), leave=False) as t:\n",
    "        for i, (src, tgt) in enumerate(iterator):\n",
    "            src_mask = (src != PAD_IDX).to(device)\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(src, tgt, src_mask)\n",
    "\n",
    "            loss = loss_fn(output[1:].view(-1, output.shape[2]),\n",
    "                           tgt[1:].view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if clip is not None:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            avg_loss = epoch_loss / (i + 1)\n",
    "            t.set_postfix(loss='{:05.3f}'.format(avg_loss),\n",
    "                          ppl='{:05.3f}'.format(np.exp(avg_loss)))\n",
    "            t.update()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ],
   "id": "19ea95cc0bb2f756",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T22:30:52.330054Z",
     "start_time": "2024-06-27T22:30:52.313417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model, iterator, loss_fn, device):\n",
    "    model.eval()\n",
    "    if model.device != device:\n",
    "        model = model.to(device)\n",
    "\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(iterator), leave=False) as t:\n",
    "            for i, (src, tgt) in enumerate(iterator):\n",
    "                src_mask = (src != PAD_IDX).to(device)\n",
    "                src = src.to(device)\n",
    "                tgt = tgt.to(device)\n",
    "\n",
    "                output = model(src, tgt, src_mask, teacher_forcing_ratio=0)\n",
    "                loss = loss_fn(output[1:].view(-1, output.shape[2]),\n",
    "                               tgt[1:].view(-1))\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                avg_loss = epoch_loss / (i + 1)\n",
    "                t.set_postfix(loss='{:05.3f}'.format(avg_loss),\n",
    "                              ppl='{:05.3f}'.format(np.exp(avg_loss)))\n",
    "                t.update()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ],
   "id": "ef498dee2a96add",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T22:30:52.680897Z",
     "start_time": "2024-06-27T22:30:52.664346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_params(model, return_int=False):\n",
    "    params = sum([torch.prod(torch.tensor(x.shape)).item() for x in model.parameters() if x.requires_grad])\n",
    "    if return_int:\n",
    "        return params\n",
    "    else:\n",
    "        print(\"There are {:,} trainable parameters in this model.\".format(params))"
   ],
   "id": "6cd123edca1f8a03",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Time",
   "id": "8d85eda9280584fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-27T22:31:22.218502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_params(seq2seq)\n",
    "enc_optim = torch.optim.AdamW(seq2seq.encoder.parameters(), lr=1e-4)\n",
    "dec_optim = torch.optim.AdamW(seq2seq.decoder.parameters(), lr=1e-4)\n",
    "optims = MultipleOptimizer(enc_optim, dec_optim)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "N_EPOCHS = 20\n",
    "CLIP = 10  # clipping value, or None to prevent gradient clipping\n",
    "EARLY_STOPPING_EPOCHS = 2\n",
    "\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    print(f\"Creating directory {SAVE_DIR}\")\n",
    "    os.mkdir(SAVE_DIR)\n",
    "\n",
    "model_path = os.path.join(SAVE_DIR, 'bahdanau_en_fr.pt')\n",
    "bahdanau_metrics = {}\n",
    "best_valid_loss = float(\"inf\")\n",
    "early_stopping_count = 0\n",
    "for epoch in tqdm(range(N_EPOCHS), leave=False, desc=\"Epoch\"):\n",
    "    train_loss = train(seq2seq, train_iter, optims, loss_fn, device, clip=CLIP)\n",
    "    valid_loss = evaluate(seq2seq, valid_iter, loss_fn, device)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        tqdm.write(f\"Checkpointing at epoch {epoch + 1}\")\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(seq2seq.state_dict(), model_path)\n",
    "        early_stopping_count = 0\n",
    "    else:\n",
    "        early_stopping_count += 1\n",
    "\n",
    "    bahdanau_metrics[epoch + 1] = dict(\n",
    "        train_loss=train_loss,\n",
    "        train_ppl=np.exp(train_loss),\n",
    "        valid_loss=valid_loss,\n",
    "        valid_ppl=np.exp(valid_loss)\n",
    "    )\n",
    "\n",
    "    if early_stopping_count == EARLY_STOPPING_EPOCHS:\n",
    "        tqdm.write(f\"Early stopping triggered in epoch {epoch + 1}\")\n",
    "        break"
   ],
   "id": "b6129f57d3bc8c07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 21,449,906 trainable parameters in this model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2395d22eea64bbfaa4c4bebe285326d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/6757 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c77eb8af944c48539c238db3c338e2a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/853 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f351e3311dcb4874a7ceba74d4abdfaa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpointing at epoch 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/6757 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db68c87eac354df18e0df09cb4f2c476"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "seq2seq.load_state_dict(torch.load(model_path, map_location=device))\n",
    "bahdanau_metrics_df = pd.DataFrame(bahdanau_metrics).T\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(bahdanau_metrics_df['train_loss'], label=\"Training\", color='gray', linestyle='solid', lw=2.5)\n",
    "plt.plot(bahdanau_metrics_df['valid_loss'], label=\"Validation\", color='gray', linestyle='dashed', lw=2.5)\n",
    "plt.legend()\n",
    "plt.title(\"Bahdanau Attention: Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(bahdanau_metrics_df['train_ppl'], label=\"Training\", color='gray', linestyle='solid', lw=2.5)\n",
    "plt.plot(bahdanau_metrics_df['valid_ppl'], label=\"Validation\", color='gray', linestyle='dashed', lw=2.5)\n",
    "plt.legend()\n",
    "plt.title(\"Bahdanau Attention: Perplexity\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.show()"
   ],
   "id": "e43b471037463b60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Transformer time\n",
    "!<img src=\"transformer_architecture.png\" width=500>."
   ],
   "id": "417fd701dd641a3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
    "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
    "        Here, we use sine and cosine functions of different frequencies.\n",
    "    .. math::\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout_p=0.1, max_len=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.) / d_model)) # todo change step to 3, what happens?\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        # pe.shape -> (l, 1, d)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, num_attention_heads,\n",
    "                 num_encoder_layers, num_decoder_layers, dim_feedforward,\n",
    "                 max_seq_length, pos_dropout, transformer_dropout):\n",
    "        super().__init__(self)\n",
    "        self.d_model = d_model\n",
    "        self.embed_src = nn.Embedding(input_dim, d_model)\n",
    "        self.embed_tgt = nn.Embedding(output_dim, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, pos_dropout, max_seq_length)\n",
    "        self.transformer = nn.Transformer(d_model, num_attention_heads, num_encoder_layers,\n",
    "                                          num_decoder_layers, dim_feedforward, transformer_dropout)\n",
    "        self.output = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "    def forward(self, src, tgt,\n",
    "                src_mask=None, tgt_mask=None,\n",
    "                src_key_padding_mask=None, tgt_key_padding_mask=None,\n",
    "                memory_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the Transformer model.\n",
    "        \n",
    "        The key_padding_masks are square because In self-attention, each token in the sequence attends to every other token, including itself. This requires a square matrix where each element (i, j) represents whether token i should attend to token j.\n",
    "    \n",
    "        :param src: Tensor of shape (S, N), where S is the source sequence length and N is the batch size.\n",
    "                    This tensor contains the indices of the source sequence tokens in the source vocabulary.\n",
    "        :param tgt: Tensor of shape (T, N), where T is the target sequence length and N is the batch size.\n",
    "                    This tensor contains the indices of the target sequence tokens in the target vocabulary.\n",
    "        :param src_mask: Tensor of shape (S, S) or None. The source mask is used to mask out positions in the source sequence.\n",
    "                         Typically used for preventing attention to future tokens in self-attention.\n",
    "        :param tgt_mask: Tensor of shape (T, T) or None. The target mask is used to mask out positions in the target sequence.\n",
    "                         Typically used for preventing attention to future tokens in self-attention.\n",
    "        :param src_key_padding_mask: Tensor of shape (N, S) or None. The source key padding mask is used to mask out padding tokens\n",
    "                                     in the source sequences, ensuring they do not affect the attention mechanism.\n",
    "        :param tgt_key_padding_mask: Tensor of shape (N, T) or None. The target key padding mask is used to mask out padding tokens\n",
    "                                     in the target sequences, ensuring they do not affect the attention mechanism.\n",
    "        :param memory_key_padding_mask: Tensor of shape (N, S) or None. The memory key padding mask is used to mask out padding tokens\n",
    "                                        in the encoder outputs (memory) during the decoding process, ensuring they do not affect the attention mechanism.\n",
    "        :return: Tensor of shape (T, N, E), where T is the target sequence length, N is the batch size, and E is the embedding dimension.\n",
    "                 This tensor contains the output logits of the Transformer model for each token in the target sequence.\n",
    "        \"\"\"\n",
    "        src_embedded = self.embed_src(src) * np.sqrt(self.d_model)\n",
    "        tgt_embedded = self.embed_tgt(tgt) * np.sqrt(self.d_model)\n",
    "        \n",
    "        src_embedded = self.pos_enc(src_embedded)\n",
    "        tgt_embedded = self.pos_enc(tgt_embedded)\n",
    "        \n",
    "        output = self.transformer(src_embedded, tgt_embedded,\n",
    "                                  tgt_mask=tgt_mask,\n",
    "                                  src_key_padding_mask=src_key_padding_mask,\n",
    "                                  tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                  memory_key_padding_mask=memory_key_padding_mask)\n",
    "        \n",
    "        return self.output(output)"
   ],
   "id": "532edb97b546b5b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "transformer = TransformerModel(input_dim=len(en_vocab), output_dim=len(fr_vocab), d_model=256,\n",
    "                               num_attention_heads=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048,\n",
    "                               max_seq_length=32, pos_dropout=0.15, transformer_dropout=0.3\n",
    "                               )\n",
    "transformer = transformer.to(device)"
   ],
   "id": "4cdac9af3f8e4f5f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
